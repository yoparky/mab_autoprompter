# Incorporated data: HotpotQA, SQUaD, ARC, BBH, PiQA, LogiQA, DROP, JSONSchemaBench, TwinDoc

# requires .env file to run openai api

# Meta parameters, prompts config
parameters_file_path: "./config/prompt_parameters_v1.yaml"
llm_prompt_config_path: "./config/llm_prompt_config.yaml"

# Dataset config
# dataset_file_path: "./data/data_sets/squad-train-v2.0.json"
dataset_file_path: "./data/data_sets/gsm8k_train.jsonl"
dataset_cut_start_index: 300 # leave empty to start from 0
dataset_cut_end_index: 310 # leave empty to go to end
# Dataset split
test_ratio: 0.5
train_ratio: 0.3
val_ratio: 0.2

# Results config
output_file_path: "./results/results_dataframe.csv"
output_tree_path: "./results/prompt_tree"

# tree control
tree_generative_iteration: 2
children_generation_per_node_expansion: 4

# linear control
loop_iteration_count: 5

# Concurrency 
max_concurrent_calls: 4 # Semaphore count

# heap
heap_size: 3

# max n-shots
max_n_shots: 3
# hard question sampling count
hard_sample_count: 5
# param sampling count per generation
max_param_sample_count: 2

# llm provider options
llm_provider: "openai" # "anthropic" "google" "ollama" "vllm" "url"
llm_provider_model: "gpt-4o-mini"
llm_provider_temperature: 0.0

# llm_provider: "ollama" # "anthropic" "google" "ollama" "vllm" "url"
# llm_provider_model: "qwen3:1.7b"
# llm_provider_temperature: 0.0

llm_generator: "openai"
llm_generator_model: "gpt-4o" #"o3-2025-04-16"
llm_generator_temperature: 0.7

llm_advanced_generator: "openai" 
llm_advanced_generator_model: "o3-2025-04-16"
llm_advanced_generator_temperature: 1.0
